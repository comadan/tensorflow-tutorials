{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec SkipGram Model Example\n",
    "This notebook implements a skipgram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class\n",
    "\n",
    "We can use classes to encapsulate tensorflow models. The below skeleton shows one way of using classes with tensorflow.\n",
    "\n",
    "The Model class has methods, variables, and properties that capture both the graph and the tensorflow session\n",
    "\n",
    "#### Tensorflow Graph\n",
    "\n",
    "A tensorflow graph is a computational graph of different tensorflow operations. It defines the computation and how different operations and tensors relate, but it doesn't actually do the computation or store the values of the variables. All of that magic happens within the tensorflow session.\n",
    "\n",
    "#### Tensorflow Session\n",
    "\n",
    "A tensorflow session is the context where values for tensorflow variables are instantiated and computations are run. So if you are saving a model's weights, you are actually saving the weights of the tensorflow session. If you are loading a model's weights, you need to load them into a session. When variables are initialized, that has to happen within a session. In a way, the graph is stateless. State is stored in sessions. The session also takes care of running computations, so if you are running training, those need to be run in the session.\n",
    "\n",
    "A session is instantiated with a graph, typically the current default graph. A session is only able to run computations on the graph that is tied to the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepLearningModel():\n",
    "    def __init__():\n",
    "        return\n",
    "    \n",
    "    def gen_uniform_random_weights(self, k_out, k_in, scale, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Returns weights of shape (k_in, k_out) initialized between [-scale, scale]\n",
    "        \"\"\"\n",
    "        return ((np.random.rand(k_in, k_out) * 2 - 1) * scale).astype(dtype)\n",
    "\n",
    "    def gen_random_weights_tanh(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = (6. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_random_weights_sigmoid(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = 4. * (6. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_random_weights_reLu(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = (2. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_biases(self, k, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize biases as zero.\n",
    "        \"\"\"\n",
    "        return np.zeros((k, ), dtype=dtype)\n",
    "    \n",
    "    def clip_gradient(self, grad, magnitude=1.0):\n",
    "        \"\"\"returns a clipped gradient, where it is between [-magnitude and magnitude]\"\"\"\n",
    "        magnitude = abs(magnitude)\n",
    "        return tf.maximum(tf.minimum(grad, magnitude), - magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "There are several options for cost\n",
    "\n",
    "- Negative Sampling\n",
    "- NCE Loss\n",
    "- Sampled Soft Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(DeepLearningModel):\n",
    "    \"\"\"\n",
    "    Tutorial Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k_embedding, n_embeddings, n_negative_sample):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            num_layers: number of hidden layers\n",
    "            k_hidden: number of units in the hidden layers\n",
    "            k_embedding: dimensionality of the input\n",
    "            k_softmax: dimensionality of the output layer\n",
    "        \"\"\"\n",
    "        self._graph = None\n",
    "        self._session = None\n",
    "        self.k_embedding = k_embedding\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.n_negative_sample = n_negative_sample\n",
    "        \n",
    "        self._merged_training_summary = None\n",
    "        self._merged_validation_summary = None\n",
    "    \n",
    "    \n",
    "    def load_model(self, model_filename):\n",
    "        with self.graph.as_default():\n",
    "            model_saver = tf.train.Saver()\n",
    "        \n",
    "        self._session = tf.Session(graph=self.graph)\n",
    "        model_saver.restore(self._session, model_filename)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, model_filename):\n",
    "        with self.graph.as_default():\n",
    "            model_saver = tf.train.Saver()\n",
    "            \n",
    "        model_saver.save(self.session, model_filename)\n",
    "        \n",
    "    def create_graph(self):\n",
    "        self.cells = {}\n",
    "                        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.X_center_word = tf.placeholder(tf.int32, shape=(None), name=\"X_center_word\")\n",
    "                self.X_outer_word = tf.placeholder(tf.int32, shape=(None), name=\"X_outer_word\")\n",
    "                self.X_negative_sample = tf.placeholder(tf.int32, shape=(None, self.n_negative_sample), name=\"X_outer_word\")\n",
    "                self.learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "            \n",
    "            with tf.name_scope(\"embeddings\"):\n",
    "                self.center_word_embeddings = tf.Variable(np.random.rand(self.n_embeddings, self.k_embedding).astype(np.float32), dtype=tf.float32, name=\"center_word_embeddings\")\n",
    "                self.outer_word_embeddings = tf.Variable(np.random.rand(self.n_embeddings, self.k_embedding).astype(np.float32), dtype=tf.float32, name=\"outer_word_embeddings\")\n",
    "                self.outer_word_biases = tf.Variable(np.zeros(self.n_embeddings, dtype=np.float32), name=\"outer_word_biases\")\n",
    "                \n",
    "                self.center_word_embedding_lookup = tf.nn.embedding_lookup(self.center_word_embeddings, self.X_center_word)\n",
    "                self.center_word_embedding_lookup_expanded = tf.expand_dims(tf.nn.embedding_lookup(self.center_word_embeddings, self.X_center_word), axis = 1)\n",
    "                self.outer_word_embedding_lookup = tf.nn.embedding_lookup(self.outer_word_embeddings, self.X_outer_word)\n",
    "                self.outer_word_bias_lookup = tf.nn.embedding_lookup(self.outer_word_biases, self.X_outer_word)\n",
    "                self.negative_sample_embedding_lookup = tf.nn.embedding_lookup(self.outer_word_embeddings, self.X_negative_sample)\n",
    "                self.negative_sample_bias_lookup = tf.nn.embedding_lookup(self.outer_word_biases, self.X_negative_sample)\n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.objective_target = tf.sigmoid(tf.reduce_sum(tf.multiply(self.center_word_embedding_lookup, self.outer_word_embedding_lookup), axis = 1) + self.outer_word_bias_lookup)\n",
    "                self.test = tf.reduce_sum(tf.multiply(- self.center_word_embedding_lookup_expanded, self.negative_sample_embedding_lookup), axis=2)\n",
    "                self.objective_negative_sample = tf.reduce_sum(tf.sigmoid(tf.reduce_sum(tf.multiply(- self.center_word_embedding_lookup_expanded, self.negative_sample_embedding_lookup), axis=2) + self.negative_sample_bias_lookup), axis=1)\n",
    "                self.loss = tf.reduce_mean(- (self.objective_target + self.objective_negative_sample))\n",
    "            \n",
    "            \n",
    "            with tf.name_scope(\"optimization\"):\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "                self.grads_and_vars = self.optimizer.compute_gradients(self.loss)                    \n",
    "                self.clipped_grads_and_vars = [(self.clip_gradient(gv[0]), gv[1]) for gv in self.grads_and_vars]\n",
    "                self.update_op = self.optimizer.apply_gradients(self.clipped_grads_and_vars)\n",
    "\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        return\n",
    "            \n",
    "\n",
    "    def write_graph(self):\n",
    "        self.tensorboard_writer.add_graph(self.graph)\n",
    "        return\n",
    "    \n",
    "    def init_model(self, adam_beta1=0.9, adam_beta2=0.999):\n",
    "        self.session.run(self.init_op)\n",
    "    \n",
    "    @property\n",
    "    def graph(self):\n",
    "        if self._graph is None:\n",
    "            self.create_graph()\n",
    "        return self._graph\n",
    "    \n",
    "    @property\n",
    "    def session(self):\n",
    "        if self._session is None:\n",
    "            self._session = tf.Session(graph=self.graph)\n",
    "        return self._session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_embedding=2\n",
    "n_embeddings=100\n",
    "n_negative_sample= 8\n",
    "\n",
    "learning_rate = .1\n",
    "minibatch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a model instance with 2 hidden layers and 10 hidden units.\n",
    "\n",
    "model = SkipGram(k_embedding, n_embeddings, n_negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x1169dbfd0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "center_word = np.random.randint(0, n_embeddings, size=(minibatch_size, ))\n",
    "target_word = np.random.randint(0, n_embeddings, size=(minibatch_size, ))\n",
    "negative_samples = np.random.randint(0, n_embeddings, size=(minibatch_size, n_negative_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02730493,  0.48873454],\n",
       "       [ 0.09974992,  0.55979162],\n",
       "       [ 0.40553328,  0.76625854]], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.center_word_embedding_lookup, feed_dict={model.X_center_word: center_word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68758434,  0.68085098],\n",
       "       [ 0.19688462,  0.55540323],\n",
       "       [ 0.86161876,  0.52179176]], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.outer_word_embedding_lookup, feed_dict={model.X_outer_word: target_word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8, 2)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.negative_sample_embedding_lookup, feed_dict={model.X_negative_sample: negative_samples}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.negative_sample_bias_lookup, feed_dict={model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58698851,  0.58189303,  0.67901361], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.objective_target, feed_dict={model.X_center_word: center_word, \n",
    "                                                     model.X_outer_word: target_word,\n",
    "                                                     model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.60950661,  3.60912609,  2.98638296], dtype=float32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.objective_negative_sample, feed_dict={model.X_center_word: center_word, \n",
    "                                                              model.X_outer_word: target_word,\n",
    "                                                              model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0176368"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.loss, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0108158 , -0.04481015, -0.18595314, -0.26334992, -0.2777091 ,\n",
       "        -0.24344674, -0.17242643, -0.37317902],\n",
       "       [-0.2641167 , -0.40357202, -0.33077899, -0.0796399 , -0.13241777,\n",
       "        -0.0303914 , -0.065114  , -0.26923794],\n",
       "       [-0.65398818, -0.38898832, -0.37631741, -0.83073562, -0.37631741,\n",
       "        -0.74791992, -0.4793309 , -0.3263647 ]], dtype=float32)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.test, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.session.run(model.update_op, feed_dict={model.X_center_word: center_word, \n",
    "                                              model.X_outer_word: target_word,\n",
    "                                              model.X_negative_sample: negative_samples,\n",
    "                                              model.learning_rate: learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.15414048e-04,  -3.49701159e-02,  -1.76525638e-01,\n",
       "         -2.40010083e-01,  -2.55895585e-01,  -2.20351174e-01,\n",
       "         -1.48479238e-01,  -3.37162435e-01],\n",
       "       [ -2.54901946e-01,  -3.63368511e-01,  -3.02406490e-01,\n",
       "         -6.83926493e-02,  -9.87522975e-02,  -1.72795914e-02,\n",
       "         -4.26510386e-02,  -2.47127593e-01],\n",
       "       [ -6.22921228e-01,  -3.63044143e-01,  -3.39324653e-01,\n",
       "         -7.97489941e-01,  -3.39324653e-01,  -7.14577079e-01,\n",
       "         -4.60051268e-01,  -3.00982594e-01]], dtype=float32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.test, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0819592"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.loss, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.session.run(model.update_op, feed_dict={model.X_center_word: center_word, \n",
    "                                              model.X_outer_word: target_word,\n",
    "                                              model.X_negative_sample: negative_samples,\n",
    "                                              model.learning_rate: learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00951132, -0.02536188, -0.16742112, -0.21698518, -0.23441185,\n",
       "        -0.19755943, -0.12478894, -0.30147463],\n",
       "       [-0.24609254, -0.32355672, -0.27441859, -0.05740866, -0.06616244,\n",
       "        -0.00460211, -0.02040374, -0.22538079],\n",
       "       [-0.59256208, -0.33774155, -0.30386877, -0.76500458, -0.30386877,\n",
       "        -0.68195629, -0.44153962, -0.27621585]], dtype=float32)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.test, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.145658"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.loss, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woot! It's learning!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
