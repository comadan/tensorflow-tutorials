{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec SkipGram Model Example\n",
    "This notebook implements a skipgram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class\n",
    "\n",
    "We can use classes to encapsulate tensorflow models. The below skeleton shows one way of using classes with tensorflow.\n",
    "\n",
    "The Model class has methods, variables, and properties that capture both the graph and the tensorflow session\n",
    "\n",
    "#### Tensorflow Graph\n",
    "\n",
    "A tensorflow graph is a computational graph of different tensorflow operations. It defines the computation and how different operations and tensors relate, but it doesn't actually do the computation or store the values of the variables. All of that magic happens within the tensorflow session.\n",
    "\n",
    "#### Tensorflow Session\n",
    "\n",
    "A tensorflow session is the context where values for tensorflow variables are instantiated and computations are run. So if you are saving a model's weights, you are actually saving the weights of the tensorflow session. If you are loading a model's weights, you need to load them into a session. When variables are initialized, that has to happen within a session. In a way, the graph is stateless. State is stored in sessions. The session also takes care of running computations, so if you are running training, those need to be run in the session.\n",
    "\n",
    "A session is instantiated with a graph, typically the current default graph. A session is only able to run computations on the graph that is tied to the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepLearningModel():\n",
    "    def __init__():\n",
    "        return\n",
    "    \n",
    "    def gen_uniform_random_weights(self, k_out, k_in, scale, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Returns weights of shape (k_in, k_out) initialized between [-scale, scale]\n",
    "        \"\"\"\n",
    "        return ((np.random.rand(k_in, k_out) * 2 - 1) * scale).astype(dtype)\n",
    "\n",
    "    def gen_random_weights_tanh(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = (6. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_random_weights_sigmoid(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = 4. * (6. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_random_weights_reLu(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = (2. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_biases(self, k, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize biases as zero.\n",
    "        \"\"\"\n",
    "        return np.zeros((k, ), dtype=dtype)\n",
    "    \n",
    "    def clip_gradient(self, grad, magnitude=1.0):\n",
    "        \"\"\"returns a clipped gradient, where it is between [-magnitude and magnitude]\"\"\"\n",
    "        magnitude = abs(magnitude)\n",
    "        return tf.maximum(tf.minimum(grad, magnitude), - magnitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "There are several options for cost\n",
    "\n",
    "- Negative Sampling\n",
    "- NCE Loss\n",
    "- Sampled Soft Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(DeepLearningModel):\n",
    "    \"\"\"\n",
    "    Tutorial Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k_embedding, n_embeddings, n_negative_sample):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            num_layers: number of hidden layers\n",
    "            k_hidden: number of units in the hidden layers\n",
    "            k_embedding: dimensionality of the input\n",
    "            k_softmax: dimensionality of the output layer\n",
    "        \"\"\"\n",
    "        self._graph = None\n",
    "        self._session = None\n",
    "        self.k_embedding = k_embedding\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.n_negative_sample = n_negative_sample\n",
    "        \n",
    "        self._merged_training_summary = None\n",
    "        self._merged_validation_summary = None\n",
    "    \n",
    "    \n",
    "    def load_model(self, model_filename):\n",
    "        with self.graph.as_default():\n",
    "            model_saver = tf.train.Saver()\n",
    "        \n",
    "        self._session = tf.Session(graph=self.graph)\n",
    "        model_saver.restore(self._session, model_filename)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, model_filename):\n",
    "        with self.graph.as_default():\n",
    "            model_saver = tf.train.Saver()\n",
    "            \n",
    "        model_saver.save(self.session, model_filename)\n",
    "        \n",
    "    def create_graph(self):\n",
    "        self.cells = {}\n",
    "                        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.X_center_word = tf.placeholder(tf.int32, shape=(None), name=\"X_center_word\")\n",
    "                self.X_outer_word = tf.placeholder(tf.int32, shape=(None), name=\"X_outer_word\")\n",
    "                self.X_negative_sample = tf.placeholder(tf.int32, shape=(None, self.n_negative_sample), name=\"X_outer_word\")\n",
    "                self.learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "            \n",
    "            with tf.name_scope(\"embeddings\"):\n",
    "                self.center_word_embeddings = tf.Variable(self.gen_uniform_random_weights(self.k_embedding, self.n_embeddings, .01), dtype=tf.float32, name=\"center_word_embeddings\")\n",
    "                self.outer_word_embeddings = tf.Variable(self.gen_uniform_random_weights(self.k_embedding, self.n_embeddings, .01), dtype=tf.float32, name=\"outer_word_embeddings\")\n",
    "                self.outer_word_biases = tf.Variable(np.zeros(self.n_embeddings, dtype=np.float32), name=\"outer_word_biases\")\n",
    "                \n",
    "                self.center_word_embedding_lookup = tf.nn.embedding_lookup(self.center_word_embeddings, self.X_center_word)\n",
    "                self.center_word_embedding_lookup_expanded = tf.expand_dims(tf.nn.embedding_lookup(self.center_word_embeddings, self.X_center_word), axis = 1)\n",
    "                self.outer_word_embedding_lookup = tf.nn.embedding_lookup(self.outer_word_embeddings, self.X_outer_word)\n",
    "                self.outer_word_bias_lookup = tf.nn.embedding_lookup(self.outer_word_biases, self.X_outer_word)\n",
    "                self.negative_sample_embedding_lookup = tf.nn.embedding_lookup(self.outer_word_embeddings, self.X_negative_sample)\n",
    "                self.negative_sample_bias_lookup = tf.nn.embedding_lookup(self.outer_word_biases, self.X_negative_sample)\n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"loss\"):\n",
    "                self.objective_target = tf.sigmoid(tf.reduce_sum(tf.multiply(self.center_word_embedding_lookup, self.outer_word_embedding_lookup), axis = 1) + self.outer_word_bias_lookup)\n",
    "                self.test = tf.reduce_sum(tf.multiply(- self.center_word_embedding_lookup_expanded, self.negative_sample_embedding_lookup), axis=2)\n",
    "                self.objective_negative_sample = tf.reduce_sum(tf.sigmoid(tf.reduce_sum(tf.multiply(- self.center_word_embedding_lookup_expanded, self.negative_sample_embedding_lookup), axis=2) + self.negative_sample_bias_lookup), axis=1)\n",
    "                self.loss = tf.reduce_mean(- (self.objective_target + self.objective_negative_sample))\n",
    "            \n",
    "            \n",
    "            with tf.name_scope(\"optimization\"):\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "                self.grads_and_vars = self.optimizer.compute_gradients(self.loss)                    \n",
    "                self.clipped_grads_and_vars = [(self.clip_gradient(gv[0]), gv[1]) for gv in self.grads_and_vars]\n",
    "                self.update_op = self.optimizer.apply_gradients(self.clipped_grads_and_vars)\n",
    "\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        return\n",
    "            \n",
    "\n",
    "    def write_graph(self):\n",
    "        self.tensorboard_writer.add_graph(self.graph)\n",
    "        return\n",
    "    \n",
    "    def init_model(self, adam_beta1=0.9, adam_beta2=0.999):\n",
    "        self.session.run(self.init_op)\n",
    "        \n",
    "    def train_model(self, center_word, outer_word, negative_samples, learning_rate):\n",
    "        self.session.run(self.update_op, feed_dict={self.X_center_word: center_word, \n",
    "                                                    self.X_outer_word: outer_word,\n",
    "                                                    self.X_negative_sample: negative_samples,\n",
    "                                                    self.learning_rate: learning_rate})\n",
    "    \n",
    "    @property\n",
    "    def graph(self):\n",
    "        if self._graph is None:\n",
    "            self.create_graph()\n",
    "        return self._graph\n",
    "    \n",
    "    @property\n",
    "    def session(self):\n",
    "        if self._session is None:\n",
    "            self._session = tf.Session(graph=self.graph)\n",
    "        return self._session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_embedding=2\n",
    "n_embeddings=100\n",
    "n_negative_sample= 8\n",
    "\n",
    "learning_rate = .1\n",
    "minibatch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a model instance with 2 hidden layers and 10 hidden units.\n",
    "\n",
    "model = SkipGramModel(k_embedding, n_embeddings, n_negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x1160985d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "center_word = np.random.randint(0, n_embeddings, size=(minibatch_size, ))\n",
    "target_word = np.random.randint(0, n_embeddings, size=(minibatch_size, ))\n",
    "negative_samples = np.random.randint(0, n_embeddings, size=(minibatch_size, n_negative_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0065446 , -0.00950536],\n",
       "       [ 0.00822226, -0.00507754],\n",
       "       [-0.00338755, -0.00528418]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.center_word_embedding_lookup, feed_dict={model.X_center_word: center_word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00790009,  0.00605009],\n",
       "       [-0.00185049, -0.00462213],\n",
       "       [ 0.00253445, -0.00010422]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.outer_word_embedding_lookup, feed_dict={model.X_outer_word: target_word})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.negative_sample_embedding_lookup, feed_dict={model.X_negative_sample: negative_samples}).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.negative_sample_bias_lookup, feed_dict={model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4999727 ,  0.50000209,  0.49999797], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.objective_target, feed_dict={model.X_center_word: center_word, \n",
    "                                                     model.X_outer_word: target_word,\n",
    "                                                     model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.99995136,  4.00007486,  3.99999094], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.objective_negative_sample, feed_dict={model.X_center_word: center_word, \n",
    "                                                              model.X_outer_word: target_word,\n",
    "                                                              model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.4999967"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.loss, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.06750333e-04,  -3.34709657e-05,   9.25614877e-05,\n",
       "         -7.78011672e-05,  -6.72503447e-05,  -1.48597010e-05,\n",
       "         -2.35932239e-05,   3.65822634e-05],\n",
       "       [  1.01353304e-04,   7.95730884e-05,   2.18993137e-05,\n",
       "          6.24393579e-05,   4.58735376e-05,  -1.35509563e-05,\n",
       "          3.40296720e-05,  -3.29307622e-05],\n",
       "       [  5.20781396e-06,  -5.51463745e-06,  -7.88088000e-05,\n",
       "          3.43019337e-06,   2.34720519e-05,   2.09616555e-05,\n",
       "          5.20781396e-06,  -9.92318382e-06]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.test, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.session.run(model.update_op, feed_dict={model.X_center_word: center_word, \n",
    "                                              model.X_outer_word: target_word,\n",
    "                                              model.X_negative_sample: negative_samples,\n",
    "                                              model.learning_rate: learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.03215454e-04,  -3.08869203e-05,   9.19990125e-05,\n",
       "         -7.57898524e-05,  -6.60512596e-05,  -1.33271278e-05,\n",
       "         -1.92401858e-05,   3.82480794e-05],\n",
       "       [  1.04952516e-04,   8.25340103e-05,   2.34966028e-05,\n",
       "          6.47334527e-05,   4.78789043e-05,  -1.30141998e-05,\n",
       "          3.62582941e-05,  -3.27494417e-05],\n",
       "       [  9.29315320e-06,  -2.55616214e-06,  -7.71898267e-05,\n",
       "          4.36364917e-06,   2.47927328e-05,   2.31342838e-05,\n",
       "          9.29315320e-06,  -7.69893086e-06]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.test, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.5256953"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.loss, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.session.run(model.update_op, feed_dict={model.X_center_word: center_word, \n",
    "                                              model.X_outer_word: target_word,\n",
    "                                              model.X_negative_sample: negative_samples,\n",
    "                                              model.learning_rate: learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.98076939e-05,  -2.83791323e-05,   9.14367993e-05,\n",
       "         -7.38938688e-05,  -6.49650829e-05,  -1.18637618e-05,\n",
       "         -1.49468906e-05,   3.98861921e-05],\n",
       "       [  1.08689419e-04,   8.56162951e-05,   2.51806559e-05,\n",
       "          6.71317102e-05,   5.00161696e-05,  -1.24178350e-05,\n",
       "          3.86047905e-05,  -3.25174296e-05],\n",
       "       [  1.33104113e-05,   3.86628017e-07,  -7.56306472e-05,\n",
       "          5.29165754e-06,   2.61199821e-05,   2.53099806e-05,\n",
       "          1.33104113e-05,  -5.42772614e-06]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.test, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.5513883"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.session.run(model.loss, feed_dict={model.X_center_word: center_word, \n",
    "                                         model.X_outer_word: target_word,\n",
    "                                         model.X_negative_sample: negative_samples})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woot! It's learning!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
