{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Example\n",
    "This notebook demonstrates saving and loading a tensorflow model into and out of a class.\n",
    "\n",
    "It implements a class to store the basic skeleton of a tensorflow model, but leaves out any training or prediction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_dataset = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_dataset.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Class\n",
    "\n",
    "We can use classes to encapsulate tensorflow models. The below skeleton shows one way of using classes with tensorflow.\n",
    "\n",
    "The Model class has methods, variables, and properties that capture both the graph and the tensorflow session\n",
    "\n",
    "#### Tensorflow Graph\n",
    "\n",
    "A tensorflow graph is a computational graph of different tensorflow operations. It defines the computation and how different operations and tensors relate, but it doesn't actually do the computation or store the values of the variables. All of that magic happens within the tensorflow session.\n",
    "\n",
    "#### Tensorflow Session\n",
    "\n",
    "A tensorflow session is the context where values for tensorflow variables are instantiated and computations are run. So if you are saving a model's weights, you are actually saving the weights of the tensorflow session. If you are loading a model's weights, you need to load them into a session. When variables are initialized, that has to happen within a session. In a way, the graph is stateless. State is stored in sessions. The session also takes care of running computations, so if you are running training, those need to be run in the session.\n",
    "\n",
    "A session is instantiated with a graph, typically the current default graph. A session is only able to run computations on the graph that is tied to the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepLearningModel():\n",
    "    def __init__():\n",
    "        return\n",
    "    \n",
    "    def gen_uniform_random_weights(self, k_out, k_in, scale, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Returns weights of shape (k_in, k_out) initialized between [-scale, scale]\n",
    "        \"\"\"\n",
    "        return ((np.random.rand(k_in, k_out) * 2 - 1) * scale).astype(dtype)\n",
    "\n",
    "    def gen_random_weights_tanh(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = (6. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_random_weights_sigmoid(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = 4. * (6. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_random_weights_reLu(self, k_out, k_in, dtype=np.float32):\n",
    "        scale = (2. / (k_in + k_out)) ** .5\n",
    "        return self.gen_uniform_random_weights(k_out, k_in, scale, dtype=dtype)\n",
    "\n",
    "    def gen_biases(self, k, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize biases as zero.\n",
    "        \"\"\"\n",
    "        return np.zeros((k, ), dtype=dtype)\n",
    "    \n",
    "    def clip_gradient(self, grad, magnitude=1.0):\n",
    "        \"\"\"returns a clipped gradient, where it is between [-magnitude and magnitude]\"\"\"\n",
    "        magnitude = abs(magnitude)\n",
    "        return tf.maximum(tf.minimum(grad, magnitude), - magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecurrentNetworkModel(DeepLearningModel):\n",
    "    \"\"\"\n",
    "    Tutorial Model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, k_rnncell, k_input, time_steps):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            num_layers: number of hidden layers\n",
    "            k_hidden: number of units in the hidden layers\n",
    "            k_input: dimensionality of the input\n",
    "            k_softmax: dimensionality of the output layer\n",
    "        \"\"\"\n",
    "        self._graph = None\n",
    "        self._session = None\n",
    "        self.num_layers = num_layers\n",
    "        self.k_rnncell = k_rnncell\n",
    "        self.k_input = k_input\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        self._merged_training_summary = None\n",
    "        self._merged_validation_summary = None\n",
    "    \n",
    "    \n",
    "    def load_model(self, model_filename):\n",
    "        with self.graph.as_default():\n",
    "            model_saver = tf.train.Saver()\n",
    "        \n",
    "        self._session = tf.Session(graph=self.graph)\n",
    "        model_saver.restore(self._session, model_filename)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self, model_filename):\n",
    "        with self.graph.as_default():\n",
    "            model_saver = tf.train.Saver()\n",
    "            \n",
    "        model_saver.save(self.session, model_filename)\n",
    "        \n",
    "    def create_graph(self):\n",
    "        self.cells = {}\n",
    "                        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            with tf.name_scope(\"inputs\"):\n",
    "                self.data = tf.placeholder(tf.float32, shape=(None, self.time_steps, self.k_rnncell))\n",
    "            with tf.name_scope(\"recurrent_layers\"):\n",
    "                cells = []\n",
    "                for layer in range(1, num_layers + 1):\n",
    "                    cell = tf.contrib.rnn.BasicRNNCell(self.k_rnncell)\n",
    "                    cells.append(cell)\n",
    "                rnn_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "            \n",
    "            with tf.name_scope(\"cost\"):\n",
    "                with tf.name_scope(\"regularization\"):\n",
    "                    self.L2_reg = tf.placeholder(tf.float32, name=\"L2_reg\")\n",
    "                    for layer in range(1, self.num_layers + 1):\n",
    "                        if layer == 1:\n",
    "                            k_in = self.k_input\n",
    "                            self.cost_L2 = self.L2_reg * tf.reduce_mean(tf.square(self.W[(layer, layer - 1)]))\n",
    "                        else:\n",
    "                            k_in = self.k_hidden\n",
    "                            self.cost_L2 = self.cost_L2 + self.L2_reg * tf.reduce_mean(tf.square(self.W[(layer, layer - 1)]))\n",
    "                    self.cost_L2 = tf.identity(self.cost_L2, 'cost_L2_regularization')\n",
    "                \n",
    "                with tf.name_scope(\"error\"):\n",
    "                    self.cross_entropy_error = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.Y, logits=self.Z[\"softmax\"]))\n",
    "                self.total_cost = tf.add(self.cost_L2, self.cross_entropy_error)\n",
    "            \n",
    "            with tf.name_scope(\"optimization\"):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=self.adam_beta1, beta2=self.adam_beta2, epsilon=self.adam_epsilon)\n",
    "                self.grads_and_vars = self.optimizer.compute_gradients(self.total_cost)                    \n",
    "                self.clipped_grads_and_vars = [(self.clip_gradient(gv[0]), gv[1]) for gv in self.grads_and_vars]\n",
    "                self.update_op = self.optimizer.apply_gradients(self.clipped_grads_and_vars)\n",
    "                \n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "        return\n",
    "            \n",
    "    def create_tensorboard_summaries(self):\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope(\"summaries\"):\n",
    "                tf.summary.scalar('cross_entropy_error', self.cross_entropy_error, collections=['train'])\n",
    "                tf.summary.scalar('cost_L2_regularization', self.cost_L2, collections=['train'])\n",
    "\n",
    "                tf.summary.scalar('cross_entropy_error_validation', self.cross_entropy_error, collections=['validation'])\n",
    "\n",
    "                for layer in range(1, self.num_layers + 1):\n",
    "                    tf.summary.histogram(\"W_%i_%i\" % (layer, layer - 1), self.W[(layer, layer - 1)], collections=['train'])\n",
    "                    tf.summary.histogram(\"b_%i\" % (layer, ), self.b[layer], collections=['train'])\n",
    "                for layer in [\"softmax\"]:\n",
    "                    tf.summary.histogram(\"W_%s_%i\" % (layer, self.num_layers), self.W[(layer, self.num_layers)], collections=['train'])\n",
    "                    tf.summary.histogram(\"b_%s\" % (layer, ), self.b[layer], collections=['train'])\n",
    "\n",
    "                self._merged_training_summary = tf.summary.merge_all(key='train')\n",
    "                self._merged_validation_summary = tf.summary.merge_all(key='validation')\n",
    "        return\n",
    "    \n",
    "    def create_tensorboard_writer(self, tensorboard_directory=\"./\"):\n",
    "        \"\"\"I'm not sure if this needs to be within a session\"\"\"\n",
    "        self._tensorboard_writer = tf.summary.FileWriter(tensorboard_directory, graph=self.graph)\n",
    "    \n",
    "    def write_graph(self):\n",
    "        self.tensorboard_writer.add_graph(self.graph)\n",
    "        return\n",
    "    \n",
    "    def init_model(self, adam_beta1=0.9, adam_beta2=0.999):\n",
    "        self.session.run(self.init_op, \n",
    "                         feed_dict={\n",
    "                             self.adam_beta1: adam_beta1,\n",
    "                             self.adam_beta2: adam_beta2\n",
    "                         })\n",
    "    \n",
    "    def train_model(self, X, Y, learning_rate=1e-2, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, \n",
    "                    input_dropout_keep_prob=1.0, hidden_dropout_keep_prob=1.0,\n",
    "                    L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        learning_rate: A Tensor or a floating point value. The learning rate.\n",
    "        beta1: A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.\n",
    "        beta2: A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.\n",
    "        epsilon: A small constant for numerical stability. This epsilon is \"epsilon hat\" in the Kingma and Ba paper (in the formula just before Section 2.1), not the epsilon in Algorithm 1 of the paper.\n",
    "        \"\"\"\n",
    "        self.session.run(self.update_op,\n",
    "                         feed_dict = {\n",
    "                             self.X: X,\n",
    "                             self.Y: Y,\n",
    "                             self.learning_rate: learning_rate,\n",
    "                             self.adam_beta1: adam_beta1,\n",
    "                             self.adam_beta2: adam_beta2,\n",
    "                             self.adam_epsilon: adam_epsilon,\n",
    "                             self.input_dropout_keep_prob: input_dropout_keep_prob,\n",
    "                             self.hidden_dropout_keep_prob: hidden_dropout_keep_prob,\n",
    "                             self.L2_reg: L2_reg\n",
    "                         })\n",
    "        return\n",
    "    \n",
    "    def write_validation_summary(self, X, Y, step,\n",
    "                                 L2_reg=1e-4):\n",
    "        summary = self.session.run(self.merged_validation_summary,\n",
    "                                   feed_dict = {\n",
    "                                       self.X: X,\n",
    "                                       self.Y: Y,\n",
    "                                       self.input_dropout_keep_prob: 1.0,\n",
    "                                       self.hidden_dropout_keep_prob: 1.0,\n",
    "                                       self.L2_reg: L2_reg\n",
    "                                   })\n",
    "        self.tensorboard_writer.add_summary(summary, step)\n",
    "        return\n",
    "    \n",
    "    def write_training_summary(self, X, Y, step,\n",
    "                               L2_reg=1e-4):\n",
    "        summary = self.session.run(self.merged_training_summary,\n",
    "                                   feed_dict = {\n",
    "                                       self.X: X,\n",
    "                                       self.Y: Y,\n",
    "                                       self.input_dropout_keep_prob: 1.0,\n",
    "                                       self.hidden_dropout_keep_prob: 1.0,\n",
    "                                       self.L2_reg: L2_reg\n",
    "                                   })\n",
    "        self.tensorboard_writer.add_summary(summary, step)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def graph(self):\n",
    "        if self._graph is None:\n",
    "            self.create_graph()\n",
    "        return self._graph\n",
    "    \n",
    "    @property\n",
    "    def session(self):\n",
    "        if self._session is None:\n",
    "            self._session = tf.Session(graph=self.graph)\n",
    "        return self._session\n",
    "    \n",
    "    @property\n",
    "    def merged_training_summary(self):\n",
    "        if self._merged_training_summary is None:\n",
    "            self.create_tensorboard_summaries()\n",
    "        return self._merged_training_summary\n",
    "    \n",
    "    @property\n",
    "    def merged_validation_summary(self):\n",
    "        if self._merged_validation_summary is None:\n",
    "            self.create_tensorboard_summaries()\n",
    "        return self._merged_validation_summary\n",
    "    \n",
    "    @property\n",
    "    def tensorboard_writer(self):\n",
    "        if self._tensorboard_writer is None:\n",
    "            self.create_tensorboard_writer()\n",
    "        return self._tensorboard_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "k_rnncell = 10\n",
    "k_input = 4\n",
    "time_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a model instance with 2 hidden layers and 10 hidden units.\n",
    "\n",
    "model_a = RecurrentNetworkModel(num_layers,  k_rnncell, k_input, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "RecurrentNetworkModel instance has no attribute 'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ea45753aea6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1f537b6eb3c9>\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1f537b6eb3c9>\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                             \u001b[0mk_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_L2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2_reg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                             \u001b[0mk_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: RecurrentNetworkModel instance has no attribute 'W'"
     ]
    }
   ],
   "source": [
    "model_a.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Properties\n",
    "\n",
    "The class doesn't actually create the graph or session until the graph and session properties are called. The @property decorator functions above are used to create a graph or session if none exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a._graph is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a._session is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.merged_training_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.merged_validation_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.merged_training_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.merged_validation_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.session.run(model_a.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.session.run(model_a.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.create_tensorboard_writer(\"./tensorboard/model_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.write_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Operations\n",
    "\n",
    "The class adds some convenience functions for assigning weights. Tensorflow can only assign values to tensor variables using assignment operations, and a combination of a placeholder and assignment operation are used to allow the assignment through a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_a.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.W.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.new_W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.assign_W((1, 0), np.ones((4, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.assign_W(('softmax', 2), np.ones((10, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.assign_W((2, 1), np.ones((10, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.session.run(model_a.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_a.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.new_b_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.assign_b(1, np.ones((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.assign_b('softmax', np.ones((3, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_a.assign_b(2, np.ones((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a.session.run(model_a.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a.save_model(\"./saved_model/test_saved_model.cpkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model into a second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_b = FeedForwardSoftMaxModel(num_layers, k_hidden, k_input, k_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_b.load_model(\"./saved_model/test_saved_model.cpkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b.session.run(model_b.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c = FeedForwardSoftMaxModel(num_layers, k_hidden, k_input, k_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.session.run(model_c.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.session.run(model_c.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.create_tensorboard_writer(\"./tensorboard/model_c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.write_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.write_training_summary(iris_dataset.data, iris_dataset.target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.write_training_summary(iris_dataset.data, iris_dataset.target, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.write_training_summary(iris_dataset.data, iris_dataset.target, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.tensorboard_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.tensorboard_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_c.session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
